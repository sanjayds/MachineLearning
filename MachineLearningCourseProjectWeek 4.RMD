---
title: "Machine Learning - Course Project - Week 4"
author: "Sanjay Jain"
date: "March 27, 2017"
output: html_document
---
# Machine Learning - Course Project - Week 4
#### By Sanjay Jain - March 29,2017

## Executive Summary

It is very common to quantify how long a particular activity was done during workout but the aim of this project is to predict how "well" exercise was done. It is based on readings of the data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 

This project performs different steps like loading the training and testing data, cleaning the data, partitioning the data in training and test category, cross validation of data, creating model using different techniques on training data and predicting test data using these models, finding the model with best prediction accuracy and use that model on given 20 samples to predict how well (correctly or incorrectly) exercise was done in those 20 samples.     

## Loading and Cleaning the Raw Data

The training data for this project are available here:

[https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)

The test data are available here:

[https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

The data for this project come from this source: [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har) 

Load following packages -
```{r, results = "hide"}
library(ggplot2)
library(lubridate)
library(caret)
library(e1071)
library(randomForest)
library(rattle)
library(rpart.plot)
library(rmarkdown)
library(markdown) 
```

### Loading the data

Load training data set. Missing values are coded as blank fields. 

```{r}
cp_training <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"), na.strings = c("NA", ""))

```
After loading, check the data -

```{r}
dim(cp_training)
```

Load test cases data set. Missing values are coded as blank fields. 

```{r}
cp_testcases <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"), na.strings = c("NA", ""))

```
After loading, check the data -

```{r}
dim(cp_testcases)

```
### Cleaning the data

#### Remove columns with NA
Check column sums. If column sum is zero that means it does not have NA value. See how many columns are there with zero column sum and then select only those.

```{r}
table(colSums(is.na(cp_training)))

cp_training <- cp_training[, colSums(is.na(cp_training)) == 0]


table(colSums(is.na(cp_testcases)))


cp_testcases <- cp_testcases[, colSums(is.na(cp_testcases)) == 0]

``` 
Select only relevant predictors. First 5 columns (X, user_name, raw_timestamp_part_1, raw_timestamp_part_2 and cvtd_timestamp) do not seem to be relevant to our analysis so remove them -

```{r}
cp_training <- cp_training[, -(1:5)]

cp_testcases <- cp_testcases[, -(1:5)]
```
Check rows and columns in cleaned data. We have now total 55 columns left for our analysis.
```{r}
dim(cp_training)
dim(cp_testcases)
```

## Cross Validation

### Split training data into train and test sets -

```{r}
set.seed(12345)
inTrain <- createDataPartition(y=cp_training$classe, p= 0.7, list = FALSE)
cp_train <- cp_training[inTrain,]
cp_test <- cp_training[-inTrain,]
```
### Building models on training data set

#### Model-1 - Random forest

**Building the model**

I am using random forest method for my first model because it is considered to be one of the most accurate algorithms. Using 5 fold cross validation - 

```{r}
set.seed(12345)
mod_rf <- train(classe~., data=cp_train, method="rf", prox=TRUE, trControl=trainControl(method="cv", 5))
mod_rf

```
We see that accuracy is more than 0.99 in all mtry so this model looks good to evaluate testing data set. 

**Evaluate model on test data set**

```{r}
pred_mod_rf <- predict(mod_rf, cp_test)
confusion_Matrix <- confusionMatrix(cp_test$classe, pred_mod_rf)
confusion_Matrix
accuracy <- postResample(pred_mod_rf, cp_test$classe)
accuracy
error <- 1 - as.numeric(confusion_Matrix$overall[1])
error
```
** Estimated accuracy of random forest model is `r round(accuracy[1]*100,2)`% and out of sample error is `r round(error*100,2)`%. ** 

** Plotting Confusion matrix results **
```{r}
plot(confusion_Matrix$table, col = confusion_Matrix$byClass,  main = paste("Random Forest - Accuracy =",round(confusion_Matrix$overall[1], 3)))
```

#### Model-2 - Decision Tree (rpart)

Use the same training (cp_train) and test (cp_test) dataset as created above

**Building the Tree model**


```{r}
set.seed(12345)
mod_tree <- train(classe~., data=cp_train, method="rpart")
mod_tree$finalModel

```
** Plotting Tree model **

```{r}
fancyRpartPlot(mod_tree$finalModel)
```

**Evaluate model on test data set**

```{r} 
pred_mod_tree <- predict(mod_tree, cp_test)
 
confusion_Matrix_tree <- confusionMatrix(cp_test$classe, pred_mod_tree)

confusion_Matrix_tree

accuracy_tree <- postResample(pred_mod_tree, cp_test$classe)
accuracy_tree
error_tree <- 1 - as.numeric(confusion_Matrix_tree$overall[1])
error_tree

```
** Estimated accuracy of Tree forest model is `r round(accuracy_tree[1]*100,2)`% and out of sample error is `r round(error_tree*100,2)`%. **


#### Model-3 - Gradient Boosting Model (gbm) 

Use the same training (cp_train) and test (cp_test) dataset as created above

**Building the gbm model**


```{r}
set.seed(12345)
mod_gbm <- train(classe~., data=cp_train, method="gbm", verbose = FALSE)
mod_gbm
mod_gbm$finalModel

```

**Evaluate model on test data set**

```{r} 
pred_mod_gbm <- predict(mod_gbm, cp_test)
 
confusion_Matrix_gbm <- confusionMatrix(cp_test$classe, pred_mod_gbm)

confusion_Matrix_gbm

accuracy_gbm <- postResample(pred_mod_gbm, cp_test$classe)
accuracy_gbm
error_gbm <- 1 - as.numeric(confusion_Matrix_gbm$overall[1])
error_gbm

```
** Estimated accuracy of gbm model is `r round(accuracy_gbm[1]*100,2)`% and out of sample error is `r round(error_gbm*100,2)`%. **

**Plotting the prediction result **

```{r}
qplot(pred_mod_gbm, classe, data = cp_test, xlab = "Prediction", ylab = "Reference Classe")

```

## Model Selection Conclusion 


* Model 1 - Random Forest - Accuracy - `r round(accuracy[1]*100,2)`%, 		Error - `r round(error*100,2)`%
* Model 2 - Decision Tree - Accuracy - `r round(accuracy_tree[1]*100,2)`%, 	Error - `r round(error_tree*100,2)`%
* Model 3 - GBM		    - Accuracy - `r round(accuracy_gbm[1]*100,2)`%, 	Error - `r round(error_gbm*100,2)`% 

We see that Model 1 (Random Forest) has highest accuracy and lowest error rate. Model 2 (Decision Tree) has lowest accuracy and highest error rate. Model 3 (gbm) is in between 1 and 2. Infact it is very close to Model 1 (random forest) in terms of accuracy and error. 
Based on this conclusion, I am selecting Model 1 (random forest) to run prediction on data set with 20 samples. 

## Running selected model on Test data

Here we'll try to predict classe in the sample of 20 test cases based on random forest model (model-1).

```{r}
predict(mod_rf, cp_testcases)

```

